# **SHL Intelligent Assessment Recommendation System**
This project implements an intelligent recommendation system for SHL assessments using an advanced Retrieval-Augmented Generation (RAG) pipeline. It moves beyond simple keyword search to understand the nuanced intent of natural language queries and job descriptions, providing relevant, balanced, and context-aware assessment recommendations.

## **Key Features**

*   **Advanced RAG Pipeline:** Utilizes a sophisticated pipeline that combines semantic retrieval with intelligent re-ranking.
*   **LLM-Powered Query Understanding:** Leverages the Google Gemini API for a deep "Query Understanding Layer" that performs:
    *   **Query Decomposition:** Breaks complex user requests into multiple, focused search queries.
    *   **Structured Intent Extraction:** Analyzes queries to extract technical skills, soft skills, job seniority, and domain, applying expert rules to handle nuanced cases (e.g., Marketing vs. Sales).
*   **Strategic Re-ranking:** Employs a rule-based re-ranking engine that prioritizes practical simulations, handles seniority, and uses negative filters to improve recommendation relevance.
*   **Enriched Knowledge Base:** Programmatically infers and injects rich metadata (e.g., `assessment_style`, `primary_use`, `appropriate_for_senior`) into the vector embeddings for more powerful semantic search.
*   **FastAPI Backend:** Exposes the recommendation logic via a robust, production-ready REST API.

---

## **Project Structure**

Here is a brief explanation of the key files and folders in this project:

```
.
├── main.py              # FastAPI application server with /health and /recommend endpoints.
├── logic.py             # Core RAG pipeline logic, including all Gemini prompts and the re-ranking engine.
├── Dockerfile           # Instructions for building and deploying the application in a container.
├── requirements.txt     # A list of all Python dependencies required for the project.
│
├── data/
│   ├── assessments_with_embedding.json  # The vector database generated by embedding_gen.py.
│   ├── Gen_AI Dataset.xlsx              # The provided dataset with Train and Test sets.
│   └── shl_test_solutions_detailed.json # The raw, scraped assessment data.
│
├── evaluation/
│   ├── embedding_gen.py # Script to generate the vector embeddings from the raw data.
│   └── evaluation.py    # Script to run the RAG pipeline on the Train-Set and calculate the Mean Recall@10.
│
├── observation/
│   ├── analysis_script.py # Helper script for analyzing the Train-Set against the raw data.
│   └── (other .txt files) # Artifacts and logs from the analysis and debugging process.
│
├── scripts/
│   └── generate_submission.py # Script to run the pipeline on the Test-Set and generate the final predictions.csv.
│
└── .env                   # Local environment variables, primarily for the GEMINI_API_KEY.
```

---

## **Setup and Installation**

Follow these steps to set up the project environment.

**1. Clone the Repository**
```bash
git clone <your-repo-url>
cd <your-repo-name>
```

**2. Create and Activate a Virtual Environment**
```bash
# For Windows
python -m venv venv
.\venv\Scripts\activate

# For macOS/Linux
python3 -m venv venv
source venv/bin/activate
```

**3. Install Dependencies**
```bash
pip install -r requirements.txt
```

**4. Set Up Environment Variables**
Create a file named `.env` in the root of the project and add your Google Gemini API key:
```
GEMINI_API_KEY="AIza..."
```

---

## **How to Run the Project**

The project has several components that can be run independently.

### **Step 1: Generate the Vector Database**

This step must be run first. It reads the raw data and creates the `assessments_with_embedding.json` file.

```bash
python evaluation/embedding_gen.py
```

### **Step 2 (Optional): Run the Evaluation Script**

This script runs the RAG pipeline on the `Train-Set` to reproduce the performance score (Mean Recall@10).

```bash
python evaluation/evaluation.py
```

### **Step 3: Run the API Server Locally**

This will start the FastAPI server on `http://127.0.0.1:8000`.

```bash
uvicorn main:app --reload
```

### **Step 4: Generate the Final Submission CSV**

This script runs the pipeline on the unlabeled `Test-Set` and creates the `Sumit_Dwivedi.csv` (or `predictions.csv`) file required for submission.

```bash
python scripts/generate_submission.py
```

---

## **API Endpoints**

The API server provides two endpoints.

### **1. Health Check**

*   **Endpoint:** `/health`
*   **Method:** `GET`
*   **Description:** A simple endpoint to verify that the API server is running.
*   **Success Response (200 OK):**
    ```json
    {
      "status": "healthy"
    }
    ```

### **2. Get Recommendations**

*   **Endpoint:** `/recommend`
*   **Method:** `POST`
*   **Description:** Accepts a natural language query or job description and returns a list of the top 10 most relevant SHL assessments.
*   **Request Body:**
    ```json
    {
      "query": "I need a test for a senior java developer with leadership skills"
    }
    ```
*   **Example `curl` Command:**
    ```bash
    curl -X POST "http://127.0.0.1:8000/recommend" \
         -H "Content-Type: application/json" \
         -d '{"query": "I need a test for a senior java developer with leadership skills"}'
    ```
*   **Success Response (200 OK):**
    ```json
    {
      "recommended_assessments": [
        {
          "url": "https://www.shl.com/...",
          "name": "Core Java (Advanced Level) (New)",
          "adaptive_support": "No",
          "description": "Multi-choice test that measures...",
          "duration": 13,
          "remote_support": "Yes",
          "test_type": ["Knowledge & Skills"]
        },
        // ... more recommendations ...
      ]
    }
    ```